{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "777c642f-1257-4020-858d-bbc43891133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, how are you feeling today? You've been feeling great for so long.\n",
      "\n",
      "I'm sorry. We didn't get any business tonight. This is a bit of an embarrassment.\n",
      "\n",
      "Well, I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "Hey, you're not crying.\n",
      "\n",
      "But...\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n",
      "\n",
      "I'm sorry.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "response = generator(\"Hey, how are you feeling today?\", max_length=50, num_return_sequences=1)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e02d3a-4894-4dea-852a-5d727a18903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, how are you feeling today?\n",
      "\n",
      "\n",
      "Caleb: I'm feeling good. I've been through a lot. I've been through some ups and downs, but I feel like I've come to the right place. It's all going fine. I'm going to stay with my guys and just play like I've been doing all these years, but I just want to get the win and stay in the same position.\"\n",
      "\n",
      "\n",
      "Gonzalez: \"I want to play like I've played in my whole life, so I want to play like I've played in my entire career. You want to win at least one of those games.\"\n",
      "\n",
      "\n",
      "Gonzalez: \"You want to be an All-Star, right?\"\n",
      "\n",
      "\n",
      "Gonzalez: \"You want to win the championship.\"\n",
      "\n",
      "\n",
      "Gonzalez: \"I want to make the team. I want to play in front of my fans and the kids. And to my teammates. Everyone should know that I'm going to be here. I'm going to play. I've always been a part of this organization. It's great. I've been on the team, and I want to stay here and play.\"\n",
      "\n",
      "\n",
      "Lazar: \"Yeah, right. I'm excited.\"\n",
      "\n",
      "\n",
      "Lazar: \"But\n"
     ]
    }
   ],
   "source": [
    "response = generator(\n",
    "    \"Hey, how are you feeling today?\",\n",
    "    max_length=60,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.9,      # More creative responses\n",
    "    top_k=50,             # Limits random word choices\n",
    "    top_p=0.95,           # Limits based on cumulative probability\n",
    "    do_sample=True        # Enables sampling\n",
    ")\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e6ecd2-b146-426e-97dc-f778d680e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, how are you feeling today?\n",
      "\n",
      "\n",
      "Caleb: I'm feeling good. I've been through a lot. I've been through some ups and downs, but I feel like I've come to the right place. It's all going fine. I'm going to stay with my guys and just play like I've been doing all these years, but I just want to get the win and stay in the same position.\"\n",
      "\n",
      "\n",
      "Gonzalez: \"I want to play like I've played in my whole life, so I want to play like I've played in my entire career. You want to win at least one of those games.\"\n",
      "\n",
      "\n",
      "Gonzalez: \"You want to be an All-Star, right?\"\n",
      "\n",
      "\n",
      "Gonzalez: \"You want to win the championship.\"\n",
      "\n",
      "\n",
      "Gonzalez: \"I want to make the team. I want to play in front of my fans and the kids. And to my teammates. Everyone should know that I'm going to be here. I'm going to play. I've always been a part of this organization. It's great. I've been on the team, and I want to stay here and play.\"\n",
      "\n",
      "\n",
      "Lazar: \"Yeah, right. I'm excited.\"\n",
      "\n",
      "\n",
      "Lazar: \"But\n"
     ]
    }
   ],
   "source": [
    "response = generator(\n",
    "    \"Hey, how are you feeling today?\",\n",
    "    max_length=60,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.9,      # More creative responses\n",
    "    top_k=50,             # Limits random word choices\n",
    "    top_p=0.95,           # Limits based on cumulative probability\n",
    "    do_sample=True        # Enables sampling\n",
    ")\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5a2c3-c00f-4d5f-8a48-b849cf7eb9af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
