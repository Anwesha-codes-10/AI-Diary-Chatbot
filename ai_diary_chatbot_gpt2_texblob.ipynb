{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9536cea-47ff-45a3-b9c6-2bea30ecb8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anwes\\New folder\\llm_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user seems neutral. They said: \"I feel like no one understands me and I’m really stressed.\". Respond in a thoughtful and caring tone. \"Do you know about my sister or not?\" The user said. \"Oh, nothing too bad there.\" The user asked: \"What's a girl who doesn't understand what she's getting herself into?\" The user said: \"My sister is a pretty girl I thought I was a girl, but that was not true. She was like that.\" The user said: \"You really are an idiot if you think like that.\" The user said: \"I don't believe in ghosts and ghosts don't believe in ghosts.\" The user said: \"I am not a psychic and I don't feel like I can change my life.\" The user said: \"Oh, don't worry. I don't think you're a psychic either. You don't look like that.\" The user said: \"Don't you have a theory?\" The user said: \"No, but you are right. It isn't all that scary.\" The user said: \"Well then. I am an alien and you are an alien and you are a human and I am not sure you would do that. Why don't you explain how you are different from me?\" The user said: \"I am not that different from you either. I am just what you are.\" The user said:\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Step 1: Create the GPT-2 generator\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Step 2: Get user input (can be changed anytime)\n",
    "user_input = \"I feel like no one understands me and I’m really stressed.\"\n",
    "\n",
    "# Step 3: Analyze sentiment with TextBlob\n",
    "blob = TextBlob(user_input)\n",
    "polarity = blob.sentiment.polarity\n",
    "\n",
    "# Step 4: Detect mood\n",
    "if polarity > 0.2:\n",
    "    mood = \"happy\"\n",
    "elif polarity < -0.2:\n",
    "    mood = \"sad\"\n",
    "else:\n",
    "    mood = \"neutral\"\n",
    "\n",
    "# Step 5: Create prompt using detected mood\n",
    "prompt = f\"The user seems {mood}. They said: \\\"{user_input}\\\". Respond in a thoughtful and caring tone.\"\n",
    "\n",
    "# Step 6: Generate GPT-2 response\n",
    "response = generator(\n",
    "    prompt,\n",
    "    max_length=80,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Step 7: Display the chatbot’s reply\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086f19f5-d496-44f9-b468-7a4d7348cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotion: positive\n",
      "AI: \"I've been working on this for more than 15 years. I've spent time talking to people. I've been in the office for more than 20 years, and I've gotten really comfortable with it. People say, 'You've got to understand.' \"\n",
      "\n",
      "These feelings have been building\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from textblob import TextBlob\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "user_input = \"I feel like no one understands me and I’m really stressed.\"\n",
    "analysis = TextBlob(user_input)\n",
    "polarity = analysis.sentiment.polarity\n",
    "\n",
    "# Show polarity\n",
    "if polarity > 0:\n",
    "    emotion = \"positive\"\n",
    "elif polarity < 0:\n",
    "    emotion = \"negative\"\n",
    "else:\n",
    "    emotion = \"neutral\"\n",
    "\n",
    "print(f\"Detected Emotion: {emotion}\")\n",
    "\n",
    "response = generator(\n",
    "    f\"The user seems {emotion}. They said: \\\"{user_input}\\\" Respond in a thoughtful and caring tone:\",\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"AI:\", response[0]['generated_text'].split(\"Respond in a thoughtful and caring tone:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2b0ae1-da3d-495a-8426-bc0134cbd9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment polarity: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot:\n",
      "The user seems positive. They said: \"I feel like no one understands me and I’m really stressed.\". Respond in a thoughtful and caring tone. The user said: \"But I know you feel that way. No one knows how much we communicate. I really want to give you some sort of help and help you get better. Please go to the website. You will give me some help and I will see if I can use this. I think you have the right thing to do.\" The user says: \"Thank you. I hope your website helps people get better.\"\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "\n",
    "# Get user input\n",
    "user_input = \"I feel like no one understands me and I’m really stressed.\"\n",
    "\n",
    "# Sentiment Analysis using TextBlob\n",
    "analysis = TextBlob(user_input)\n",
    "polarity = analysis.sentiment.polarity\n",
    "\n",
    "# Print sentiment score\n",
    "print(f\"Sentiment polarity: {polarity}\")\n",
    "\n",
    "# Decide mood category based on polarity\n",
    "if polarity > 0.1:\n",
    "    mood = \"positive\"\n",
    "elif polarity < -0.1:\n",
    "    mood = \"negative\"\n",
    "else:\n",
    "    mood = \"neutral\"\n",
    "\n",
    "# Prepare a GPT-2 prompt\n",
    "prompt = f\"The user seems {mood}. They said: \\\"{user_input}\\\". Respond in a thoughtful and caring tone.\"\n",
    "\n",
    "# Generate response using GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "response = generator(\n",
    "    prompt,\n",
    "    max_length=80,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.9,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Print chatbot response\n",
    "print(\"\\nChatbot:\")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9997d29-95b2-4cd7-9ff9-ae64d127a216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
